# Portfolio

---

## Data Scientist (M.Sc)  
[Resume](/pdf/Lebenslauf_Nhi_Nguyen.pdf) | üìß [Nhi.nguyenpb@gmail.com](mailto:Nhi.nguyenpb@gmail.com)  

---

## üìù Projekte  
### **Masterarbeit: Deep Learning in EEG Classification**  
[View Poster](/pdf/Poster_Masterthesis_NhiNguyen.pdf)

In dieser Masterarbeit wird die Entwicklung generalisierbarer DL-Modelle f√ºr die Analyse von Elektroenzephalographie (EEG)-Daten untersucht, um die Herausforderungen der hohen Dimensionalit√§t und des niedrigen Signal-Rausch-Verh√§ltnisses zu √ºberwinden. Ein kleines, repr√§sentatives Subset von EEG-Datens√§tzen wurde erstellt, das die Diversit√§t einer gr√∂√üeren Sammlung abbildet und so die Generalisierbarkeit der Modelle √ºber verschiedene EEG-Dom√§nen hinweg f√∂rdert, ohne dom√§nenspezifische Anpassungen zu ben√∂tigen.

Die Arbeit analysiert 11 End-to-End-DL-Modelle, die auf 17 EEG-Datens√§tzen trainiert und getestet wurden. Ein innovativer Ansatz zur Auswahl des Subsets kombiniert Ridge-Regression mit Korrelationsanalysen, um den Median-$F_1$-Score der Modelle basierend auf diesem Subset vorherzusagen. Das final ausgew√§hlte Subset minimiert Vorhersagefehler, gew√§hrleistet Diversit√§t und vermeidet Redundanz.

Die Ergebnisse zeigen, dass kein DL-Modell eine herausragende Generalisierbarkeit √ºber alle Datens√§tze aufwies. Das ausgew√§hlte Subset, bestehend aus SEED, DREAMER-Valence, DREAMER-Arousal, EEGMAT und CHBMIT, zeigte jedoch geringe Vorhersageabweichungen und schwache Korrelationen, was seine Diversit√§t unterstreicht. Diese Arbeit liefert einen wertvollen Rahmen f√ºr die Entwicklung effizienter und generalisierbarer DL-Modelle in der EEG-Klassifikation.

**Skills**: Deep Learning (Tensorflow/ Keras), Python, Pandas, Scikit-learn, Feature Engineering, Datenvisualisierung, Projektmanagement.

![models](/img/ranking_models.png)
*Evaluation von Modellen: Leistung und Generalisierungsf√§higkeit.*

![datasets](/img/Boxplot_datasets.png) 
*Boxplot der Modellleistung √ºber verschiedene Datens√§tze.*

### **DataScience-Projekt** 
**Produktclustering und Preismodellierung bei Tankstellen**

Im Rahmen dieses Projekts entwickelten wir datengetriebene L√∂sungen zur Optimierung der **Produkterfassung** und **Preismodellierung** an **Tankstellen**. Durch **Produktclustering** mittels **Konfidenzintervallen** reduzierten wir den Erfassungsaufwand um durchschnittlich **41,1%**, w√§hrend wir die Genauigkeit beibehielten. F√ºr die **Preismodellierung** nutzten wir **Gradient Boosting** und **SHAP-Werte**, um die Preisbildung zu analysieren und zu erkl√§ren, und erzielten eine signifikante Verbesserung gegen√ºber linearen Modellen. **Datenvorverarbeitung** und **Feature Engineering**, einschlie√ülich der Integration von **Geodaten**, waren entscheidend f√ºr den Erfolg. Die Ergebnisse erm√∂glichen es der Firma, Kosten zu senken, wertvolle Einblicke in die Preisgestaltung zu gewinnen und ihre Prozesse zu optimieren. 

**Skills**: Python, Pandas, Scikit-learn, XGBoost, SHAP, statistische Analyse, Datenvisualisierung, Projektmanagement.

![werdenktwas](/img/werdenktwas.png)
*Ein Beispiel der Einsparung mithilfe der Produktclustering durch √§hnliche Preisstrukturen*

### **Computer Vision** 
**Qualit√§tssicherung von Inbusschl√ºsseln**

In diesem Projekt wurde ein Computer-Vision-Programm entwickelt, das automatisch die Ma√üe von Inbusschl√ºsseln erfasst und im Bild erg√§nzt. Ziel war es, eine kosteng√ºnstige und pr√§zise Alternative zur manuellen Qualit√§tskontrolle bereitzustellen. Das System erkennt die Werkzeuge zuverl√§ssig und misst deren Abmessungen in Millimetern. Es ist robust gegen√ºber leichten Schatten, Bildrauschen und Variationen in der Beleuchtung. Die Genauigkeit der Messung wurde anhand realer Objekte √ºberpr√ºft und zeigte eine geringe Abweichung zu manuellen Messungen.

**Skills**: Python (NumPy, Pandas, OpenCV, Matplotlib), Feature Extraction, Bildverarbeitung, Objekterkennung, Datenauswertung.

![Ausgemessene Inbusschl√ºsseln](/img/cv_inbusschl√ºsseln.png)
*Ausgemessene Inbusschl√ºsseln im Bild.*

**Deep Learning zur automatisierten Vogelartenklassifikation**

In diesem Projekt wurde ein Convolutional Neural Network (CNN) zur Klassifikation von 524 Vogelarten entwickelt. Um die Trainingsdaten zu erweitern, wurden verschiedene Augmentierungstechniken wie Anpassung von S√§ttigung und Kontrast sowie Rotation und horizontales Spiegeln angewendet.

Durch Hyperparameter-Optimierung wurde ein Modell mit f√ºnf Schichten gefunden, das eine Genauigkeit von 89 % erreichte. Mithilfe von Grad-CAM-Visualisierungen wurde die Erkl√§rbarkeit der Modellentscheidungen analysiert. Dabei zeigte sich, dass das CNN oft Schnabel, Augen oder auff√§llige Farb√ºberg√§nge zur Klassifikation heranzieht. Dies f√ºhrte gelegentlich zu Fehlklassifikationen, wenn nur ein einzelnes Merkmal betrachtet wurde. Das Projekt zeigt, dass bereits mit wenigen Schichten ein leistungsstarker Klassifikator entwickelt werden kann.

**Skills**: Python (NumPy, Pandas, TensorFlow, OpenCV, Matplotlib), CNN, Grad-CAM, GitLab f√ºr Versionskontrolle, Statistische Evaluierung der Modellleistung,  Ergebnisvisualisierung.

![Korrekt klassifizierter Vogel](/img/cv_vogel.png)
*Korrekt klassifizierter Vogel.*

![Falsch klassifizierter Vogel](/img/cv_vogel2.png)
*Falsch klassifizierter Vogel*

### **Natural Language Processing** 
**Analyse von Nachrichtenartikeln mit NLP und Web Scraping**  

Im Rahmen diese Projekts wurde einen Webcrawler entwickelt, der 1182 Artikel von *n-tv.de* heruntergeladen und in einer SQLite-Datenbank gespeichert hat. Die gesammelten Daten wurden mithilfe von NLP-Techniken analysiert, um Kategorisierungen, Wortverteilungen und Topic-Clustering zu untersuchen.  

**Hauptmethoden und Ergebnisse:**  
- **Web Scraping & Datenverarbeitung:** Extraktion von Kategorien via XPath und RegEx, Speicherung der Artikeltexte in einer Datenbank.  
- **Textanalyse:** Tokenisierung, Lemmatisierung und Berechnung der Worth√§ufigkeiten ergaben eine Gesamtlexikongr√∂√üe von 92.475 W√∂rtern, die durch Vorverarbeitung auf 78.055 reduziert wurde.  
- **√Ñhnlichkeitsanalyse:** Berechnung von Term Frequency (TF), Inverse Document Frequency (IDF) sowie Kosinus- und Sinus√§hnlichkeiten zur Dokumentklassifikation. Dabei zeigte sich, dass Wirtschaftsartikel stark durch Begriffe wie ‚ÄûProzent‚Äú oder ‚ÄûEuro‚Äú dominiert wurden.  
- **Topic Modeling:** Nutzung von *Latent Dirichlet Allocation (LDA)* und *BERTopic* zur automatisierten Themenextraktion. Dabei konnten Themen aus den Bereichen Wirtschaft und Gesellschaft identifiziert werden, wobei sich Cluster teilweise √ºberlappten.  

Die Analyse zeigte, dass klassische Kategorien aus der URL nicht immer die tats√§chlichen thematischen Schwerpunkte widerspiegeln. Verbesserungen k√∂nnten durch eine feinere Kategorisierung oder alternative Klassifikationsmethoden erzielt werden.

**Skills**: Web Scraping (XPath, RegEx, BeautifulSoup, Scrapy) , Database Management (SQLite, SQL queries), NLP (Tokenization, Lemmatisation, Stopword Removal, TF-IDF), Text Similarity Analysis, Topic Modeling (LDA, BERTopic), Data Analysis (Scikit-learn, Gensim, Pandas, NumPy), Data Visualization (Matplotlib, Seaborn, Plotly), Clustering & Classification (k-Means, Neural Topic Model)

![Clustering](/img/topic-clustering.png)
*BERTopic Modell f√ºr den ntv.de Datensatz.*


